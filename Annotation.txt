웹 크롤링은 selenium을 이용하여 웹브라우저를 실행시킨후 하는 방법과
Beautifulsoup를 사용하여 html 정보를 가져와서 저장하는 방법이 있는데
나는 selenium을 거의 대부분 사용하였고, beatuifulsoup는 페이지가 바뀔때마다 해당페이지의 
source를 가져오는 selenium의 방식을 웬지 못해줄거 같음.

필요 API : Selenium	pip install selenium
Crome web brower도 필요.  크롬 실행 - 설정 - 왼쪽 tab - Chrome 정보에서 version확인

https://chromedriver.chromium.org/downloads에서 version에 맞는 driver 다운하여
자신의 local 환경에 맞게 경로에 넣어놓음.

코드상에서 아래 내용 넣어줌.
options = webdriver.ChromeOptions()
options.add_experimental_option('excludeSwitches', ['enable-logging'])
driver = webdriver.Chrome('C://chromedriver.exe', options=options)

그 이후는 html상에서 원하는 기능의 path를 가져와서 실행하는 알고리즘을 짜 주면 되는데
워낙 다양하여 필요한 py를 사용하면 되겠다.

주로 full_xpath를 css_selector로 가져왔던거 같음.